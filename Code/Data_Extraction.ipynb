{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxQ5GlNqUMsD"
      },
      "outputs": [],
      "source": [
        "import mailbox\n",
        "import chardet\n",
        "\n",
        "# Read the contents of the uploaded file into a bytes object\n",
        "with open('/content/fradulent_emails.txt', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Detect the encoding of the file using chardet\n",
        "result = chardet.detect(data)\n",
        "encoding = result['encoding']\n",
        "\n",
        "# Decode the file contents using the detected encoding\n",
        "email_lines = data.decode(encoding).splitlines()\n",
        "\n",
        "# Create a new mailbox file\n",
        "mbox = mailbox.mbox('output.mbox')\n",
        "\n",
        "# Initialize variables for storing email headers and body\n",
        "headers = {}\n",
        "body = []\n",
        "\n",
        "# Iterate through the lines in the text file\n",
        "for line in email_lines:\n",
        "    # If the line starts with \"From \", it indicates a new email\n",
        "    if line.startswith('From '):\n",
        "        # If there is already an email being processed, add it to the mbox file\n",
        "        if headers:\n",
        "            # Combine the email headers and body into a single string\n",
        "            email = '\\n'.join([f'{k}: {v}' for k, v in headers.items()] + [''] + body)\n",
        "            # Add the email to the mbox file\n",
        "            mbox.add(mailbox.mboxMessage(email.encode('utf-8')))\n",
        "            # Reset the variables for the next email\n",
        "            headers = {}\n",
        "            body = []\n",
        "        # Parse the email headers from the \"From \" line\n",
        "        headers['From'] = line[5:].strip()\n",
        "    # If the line starts with a header field name followed by a colon, add it to the headers dictionary\n",
        "    elif ':' in line:\n",
        "        field, value = line.split(':', 1)\n",
        "        headers[field.strip()] = value.strip()\n",
        "    # Otherwise, add the line to the email body\n",
        "    else:\n",
        "        body.append(line.strip())\n",
        "\n",
        "# If there is any email left to be processed, add it to the mbox file\n",
        "if headers:\n",
        "    email = '\\n'.join([f'{k}: {v}' for k, v in headers.items()] + [''] + body)\n",
        "    mbox.add(mailbox.mboxMessage(email.encode('utf-8')))\n",
        "\n",
        "# Close the mbox file\n",
        "mbox.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld_gYTA6UmIH",
        "outputId": "90923f00-6942-40ca-eeb7-7bc74fcfd326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting whois\n",
            "  Downloading whois-0.9.27.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: whois\n",
            "  Building wheel for whois (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whois: filename=whois-0.9.27-py3-none-any.whl size=30469 sha256=53d4d74b69cb28366c75747e0b63e4e343de51d29586d8acc934309cf26f4110\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/17/36/d62eb5bcc416650499a7259d584c11e4a778de5ce0e72a8dbf\n",
            "Successfully built whois\n",
            "Installing collected packages: whois\n",
            "Successfully installed whois-0.9.27\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.4.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import mailbox\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from email.header import decode_header\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from urllib.parse import urlparse\n",
        "!pip install whois\n",
        "import whois\n",
        "import requests\n",
        "from datetime import datetime\n",
        "!pip install tldextract\n",
        "import tldextract\n",
        "import time\n",
        "import urllib.parse\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "newslSGBVWDA"
      },
      "outputs": [],
      "source": [
        "class EmailParser:\n",
        "    urlRegex = r'https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=;]*)'\n",
        "    emailRegex = r'([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
        "\n",
        "    def __init__(self, email):\n",
        "        self.email = email\n",
        "        self.__extract_email_parts()\n",
        "\n",
        "    def __extract_email_parts(self):\n",
        "        no_of_attachments = 0\n",
        "        text = str(self.email['Subject']) + \" \"\n",
        "        htmlDoc = \"\"\n",
        "        for part in self.email.walk():\n",
        "            content_type = part.get_content_type()\n",
        "            if content_type == 'text/plain':\n",
        "                text += str(part.get_payload())\n",
        "            elif content_type == 'text/html':\n",
        "                htmlDoc += part.get_payload()\n",
        "            else:\n",
        "                main_content_type = part.get_content_maintype()\n",
        "                if main_content_type in ['image','application']:\n",
        "                    no_of_attachments += 1\n",
        "        self.text, self.html, self.no_of_attachments = text, htmlDoc, no_of_attachments\n",
        "\n",
        "    def get_urls(self):\n",
        "        text_urls = set(re.findall(EmailParser.urlRegex,self.text))\n",
        "        html_urls = set(re.findall(EmailParser.urlRegex,self.html))\n",
        "        return list(text_urls.union(html_urls))\n",
        "\n",
        "    def get_email_text(self):\n",
        "        if(self.html != \"\"):\n",
        "            soup = BeautifulSoup(self.html)\n",
        "            self.text += soup.text\n",
        "        return self.text\n",
        "\n",
        "    def get_no_of_attachments(self):\n",
        "        return self.no_of_attachments\n",
        "\n",
        "    def get_sender_email_address(self):\n",
        "        sender = email['From']\n",
        "        try:\n",
        "            emails = re.findall(EmailParser.emailRegex, sender)\n",
        "        except:\n",
        "            h = decode_header(email['From'])\n",
        "            header_bytes = h[0][0]\n",
        "            sender = header_bytes.decode('ISO-8859-1')\n",
        "            emails = re.findall(EmailParser.emailRegex, sender)\n",
        "        if(len(emails) != 0):\n",
        "            return emails[len(emails)-1]\n",
        "        else:\n",
        "            return ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bveKj9YjMVfh"
      },
      "outputs": [],
      "source": [
        "class StringUtil:\n",
        "\n",
        "    dotRegex = r'\\.'\n",
        "    digitsRegex = r'[0-9]'\n",
        "    ipAddressRegex = r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}'\n",
        "    dashesRegex = r'-'\n",
        "    specialCharsRegex = r'[()@:%_\\+~#?\\=;]'\n",
        "    words = Counter()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    punctuations = ['!','@','#','$','%','^','&','*','(',')','-','_','=','+',';',':',\"'\",'\"','?','/','<','>','.',',','/','~','`']\n",
        "    def process_text(self, text):\n",
        "        text = text.lower()                    #lowercase\n",
        "        text = re.sub(r'[\\n\\t\\r]', ' ', text)  #remove escape sequences\n",
        "\n",
        "        #remove punctuations\n",
        "        punctuation = string.punctuation  # Get all punctuation marks\n",
        "        translator = str.maketrans('', '', punctuation + string.digits)  # Create a translator to remove punctuation and digits\n",
        "        text = text.translate(translator)  # Remove punctuation and digits using translate()\n",
        "\n",
        "        #tokenize and stem words\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = []\n",
        "        for w in word_tokens:\n",
        "            if w not in StringUtil.stop_words:\n",
        "                filtered_text.append(w)\n",
        "\n",
        "        #count frequency of words\n",
        "        word_counts = Counter(filtered_text)\n",
        "        stemmed_word_count = Counter()\n",
        "        for word, count in word_counts.items():\n",
        "            stemmed_word = StringUtil.stemmer.stem(word)\n",
        "            stemmed_word_count[stemmed_word] += count\n",
        "        word_counts = stemmed_word_count\n",
        "        StringUtil.words += word_counts\n",
        "        return word_counts\n",
        "    def get_most_common_words(self):\n",
        "        return StringUtil.words.most_common(1000)\n",
        "    def process_email_address(self, emailid):\n",
        "        length, noOfDots, noOfDashes, noOfSpecialChars, noOfDigits, noOfSubdomains = 0,0,0,0,0,0\n",
        "\n",
        "        length = len(emailid)\n",
        "        if(length > 0):\n",
        "            username, domain = emailid.split('@')\n",
        "            noOfSubdomains = len(re.findall(StringUtil.dotRegex,domain)) - 1\n",
        "            noOfDots = len(re.findall(StringUtil.dotRegex, username))\n",
        "            noOfSpecialChars = len(re.findall(StringUtil.specialCharsRegex, username))\n",
        "            noOfDashes = len(re.findall(StringUtil.dashesRegex, emailid))\n",
        "            noOfDigits = len(re.findall(StringUtil.digitsRegex, emailid))\n",
        "        return length, noOfDots, noOfDashes, noOfSpecialChars, noOfDigits, noOfSubdomains\n",
        "    def process_urls(self,urls):\n",
        "        api_key = '43ef6ac4c119080b1c8a0eff0ba430e99cef1f11960d3c2200b7b42ae05963a6'\n",
        "        features = {}\n",
        "        if (len(urls) == 0):\n",
        "         features['noof_urls'] = 0\n",
        "         features['length_url'] = 0\n",
        "         features['length_hostname'] = 0\n",
        "         features['ip'] = 0\n",
        "         features['ip_asn'] = 0\n",
        "         features['ip_last_analysis_date'] = 0\n",
        "         features['ip_harmless'] = 0\n",
        "         features['ip_malicious'] = 0\n",
        "         features['ip_suspicious'] = 0\n",
        "         features['ip_timeout'] = 0\n",
        "         features['ip_undetected'] = 0\n",
        "         features['ip_last_modification_date'] = 0\n",
        "         features['ip_harmless_votes'] = 0\n",
        "         features['ip_malicious_votes'] = 0\n",
        "         features['first_submission_date'] =  0\n",
        "         features['last_analysis_date'] = 0\n",
        "         features['harmless'] = 0\n",
        "         features['malicious'] = 0\n",
        "         features['suspicious'] = 0\n",
        "         features['timeout'] = 0\n",
        "         features['undetected'] = 0\n",
        "         features['last_http_response_code'] = 0\n",
        "         features['last_http_response_content_length'] = 0\n",
        "         features['last_modification_date'] = 0\n",
        "         features['harmless_votes'] = 0\n",
        "         features['malicious_votes'] = 0\n",
        "         features[' dots_number'] =  0\n",
        "         features['hyphens_number'] = 0\n",
        "         features['at_number'] = 0\n",
        "         features['qm_number'] = 0\n",
        "         features['and_number'] = 0\n",
        "         features['or_number'] = 0\n",
        "         features['eq_number'] = 0\n",
        "         features['tildes_number'] = 0\n",
        "         features['percent_number'] = 0\n",
        "         features['slash_number'] = 0\n",
        "         features['star_number'] =  0\n",
        "         features['colon_number'] = 0\n",
        "         features['comma_number'] = 0\n",
        "         features['semicolon_number'] = 0\n",
        "         features['dollar_number'] = 0\n",
        "         features['space_number'] = 0\n",
        "         features['www_number'] =  0\n",
        "         features['com_number'] = 0\n",
        "         features['http_in_path'] = 0\n",
        "         features['https_token'] = 0\n",
        "         features['ratio_digits_url'] = 0\n",
        "         features['ratio_digits_host'] = 0\n",
        "         features['punycode'] = 0\n",
        "         features['port'] =  0\n",
        "         features['tld_in_path'] = 0\n",
        "         features['tld_in_subdomain'] = 0\n",
        "         features['abnormal_subdomain'] = 0\n",
        "         features['subdomains_number'] = 0\n",
        "         features['prefix_suffix'] = 0\n",
        "         features['random_domain'] = 0\n",
        "         features['shortening_service'] = 0\n",
        "         features['path_extension'] = 0\n",
        "         features['redirections_number'] = 0\n",
        "         features['external_redirections_number'] = 0\n",
        "         features['length_words_raw'] = 0\n",
        "         features['char_repeat'] = 0\n",
        "         features['shortest_word_host'] = 0\n",
        "         features['longest_word_host'] =0\n",
        "         features['shortest_word_path'] = 0\n",
        "         features['longest_word_path'] = 0\n",
        "         features['shortest_words_raw'] = 0\n",
        "         features['longest_words_raw'] = 0\n",
        "         features['average_word_raw'] = 0\n",
        "         features['average_word_host'] = 0\n",
        "         features['average_word_path'] = 0\n",
        "         features['phish_hints'] =  0\n",
        "         features['hyperlinks_number'] = 0\n",
        "         features['ratio_intHyperlinks'] = 0\n",
        "         features['ratio_extHyperlinks'] = 0\n",
        "         features['ratio_nullHyperlinks'] = 0\n",
        "         features['extCSS_number'] = 0\n",
        "         features['submit_emails'] = 0\n",
        "         features['ratio_intMedia'] = 0\n",
        "         features['ratio_extMedia'] = 0\n",
        "         features['sfh'] = 0\n",
        "        else:\n",
        "          features['noof_urls'] = len(urls)\n",
        "          for url in urls:\n",
        "           # Extract the URL components\n",
        "            url_parts = re.findall('https?://([^/]+)([^?#]*)(\\\\?[^#]*)?(#.*)?', url)\n",
        "            if len(url_parts) == 0:\n",
        "                continue\n",
        "            host = url_parts[0][0]\n",
        "            path = url_parts[0][1]\n",
        "            query = url_parts[0][2] if len(url_parts[0]) > 2 else ''\n",
        "            fragment = url_parts[0][3] if len(url_parts[0]) > 3 else ''\n",
        "\n",
        "            # Extract the URL features\n",
        "            length_url = len(url)\n",
        "            features['length_url'] = length_url\n",
        "            length_hostname = len(host)\n",
        "            features['length_hostname'] = length_hostname\n",
        "            ip = 1 if re.match('^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$', host) else 0\n",
        "            features['ip'] =  ip\n",
        "            try:\n",
        "               ip_address = urllib.parse.urlparse(url).hostname\n",
        "            # Make API request to retrieve IP address information\n",
        "               params = {'ip': ip_address, 'apikey': api_key}\n",
        "               response = requests.get('https://www.virustotal.com/api/v3/ip_addresses', params=params)\n",
        "\n",
        "               ip_info = response.json()['data'][0]['attributes']\n",
        "               features['ip_asn'] = ip_info['asn']\n",
        "               features['ip_last_analysis_date'] = ip_info['last_analysis_date']\n",
        "               features['ip_harmless'] = ip_info['last_analysis_stats']['harmless']\n",
        "               features['ip_malicious'] = ip_info['last_analysis_stats']['malicious']\n",
        "               features['ip_suspicious'] = ip_info['last_analysis_stats']['suspicious']\n",
        "               features['ip_timeout'] = ip_info['last_analysis_stats']['timeout']\n",
        "               features['ip_undetected'] = ip_info['last_analysis_stats']['undetected']\n",
        "               features['ip_last_modification_date'] = ip_info['last_modification_date']\n",
        "               features['ip_harmless_votes'] = ip_info['total_votes']['harmless']\n",
        "               features['ip_malicious_votes'] = ip_info['total_votes']['malicious']\n",
        "\n",
        "            except:\n",
        "                 features['ip_asn'] = 0\n",
        "                 features['ip_last_analysis_date'] = 0\n",
        "                 features['ip_harmless'] = 0\n",
        "                 features['ip_malicious'] = 0\n",
        "                 features['ip_suspicious'] = 0\n",
        "                 features['ip_timeout'] = 0\n",
        "                 features['ip_undetected'] = 0\n",
        "                 features['ip_last_modification_date'] = 0\n",
        "                 features['ip_harmless_votes'] = 0\n",
        "                 features['ip_malicious_votes'] = 0\n",
        "            try:\n",
        "               url = \"https://www.virustotal.com/vtapi/v2/url/report\"\n",
        "               params = {\"apikey\": api_key , \"resource\": url}\n",
        "               response = requests.get(url, params=params)\n",
        "               url_info = response.json()['data'][0]['attributes']\n",
        "               features['first_submission_date'] =  url_info['first_submission_date']\n",
        "               features['last_analysis_date'] = url_info['last_analysis_date']\n",
        "               features['harmless'] = url_info['last_analysis_stats']['harmless']\n",
        "               features['malicious'] = url_info['last_analysis_stats']['malicious']\n",
        "               features['suspicious'] = url_info['last_analysis_stats']['suspicious']\n",
        "               features['timeout'] = url_info['last_analysis_stats']['timeout']\n",
        "               features['undetected'] = url_info['last_analysis_stats']['undetected']\n",
        "               features['last_http_response_code'] = url_info['last_http_response_code']\n",
        "               features['last_http_response_content_length'] = url_info['last_http_response_content_length']\n",
        "               features['last_modification_date'] = url_info['last_modification_date']\n",
        "               features['harmless_votes'] = url_info['total_votes']['harmless']\n",
        "               features['malicious_votes'] = url_info['total_votes']['malicious']\n",
        "            except:\n",
        "               features['first_submission_date'] =  0\n",
        "               features['last_analysis_date'] = 0\n",
        "               features['harmless'] = 0\n",
        "               features['malicious'] = 0\n",
        "               features['suspicious'] = 0\n",
        "               features['timeout'] = 0\n",
        "               features['undetected'] = 0\n",
        "               features['last_http_response_code'] = 0\n",
        "               features['last_http_response_content_length'] = 0\n",
        "               features['last_modification_date'] = 0\n",
        "               features['harmless_votes'] = 0\n",
        "               features['malicious_votes'] = 0\n",
        "            dots_number = host.count('.')\n",
        "            features[' dots_number'] =   dots_number\n",
        "            hyphens_number = host.count('-')\n",
        "            features['hyphens_number'] = hyphens_number\n",
        "            at_number = host.count('@')\n",
        "            features['at_number'] = at_number\n",
        "            qm_number = query.count('?')\n",
        "            features['qm_number'] = qm_number\n",
        "            and_number = query.count('&')\n",
        "            features['and_number'] = and_number\n",
        "            or_number = query.count('|')\n",
        "            features['or_number'] = or_number\n",
        "            eq_number = query.count('=')\n",
        "            features['eq_number'] = eq_number\n",
        "            tildes_number = query.count('~')\n",
        "            features['tildes_number'] =  tildes_number\n",
        "            percent_number = query.count('%')\n",
        "            features['percent_number'] =  percent_number\n",
        "            slash_number = path.count('/')\n",
        "            features['slash_number'] =  slash_number\n",
        "            star_number = path.count('*')\n",
        "            features['star_number'] =  star_number\n",
        "            colon_number = path.count(':')\n",
        "            features['colon_number'] =  colon_number\n",
        "            comma_number = path.count(',')\n",
        "            features['comma_number'] =  comma_number\n",
        "            semicolon_number = path.count(';')\n",
        "            features['semicolon_number'] = semicolon_number\n",
        "            dollar_number = path.count('$')\n",
        "            features['dollar_number'] = dollar_number\n",
        "            space_number = path.count(' ')\n",
        "            features['space_number'] = space_number\n",
        "            www_number = 1 if 'www' in host else 0\n",
        "            features['www_number'] =  www_number\n",
        "            com_number = 1 if '.com' in host else 0\n",
        "            features['com_number'] =  com_number\n",
        "            http_in_path = 1 if 'http' in path else 0\n",
        "            features['http_in_path'] =   http_in_path\n",
        "            https_token = 1 if re.search('https', url) else 0\n",
        "            features['https_token'] =  https_token\n",
        "            ratio_digits_url = sum(c.isdigit() for c in url) / len(url)\n",
        "            features['ratio_digits_url'] =  ratio_digits_url\n",
        "            ratio_digits_host = sum(c.isdigit() for c in host) / len(host)\n",
        "            features['ratio_digits_host'] =  ratio_digits_host\n",
        "            punycode = 1 if 'xn--' in host else 0\n",
        "            features['punycode'] =  punycode\n",
        "            port = 1 if ':' in host else 0\n",
        "            features['port'] =  port\n",
        "            tld_in_path = 1 if re.search('\\.[a-z]{2,3}/', url) else 0\n",
        "            features['tld_in_path'] = tld_in_path\n",
        "            tld_in_subdomain = 1 if re.search('\\.[a-z]{2,3}\\.', host) else 0\n",
        "            features['tld_in_subdomain'] = tld_in_subdomain\n",
        "            abnormal_subdomain = 1 if re.search('[^a-zA-Z0-9.-]', host) else 0\n",
        "            features['abnormal_subdomain'] = abnormal_subdomain\n",
        "            subdomains_number = host.count('.')\n",
        "            features['subdomains_number'] = subdomains_number\n",
        "            prefix_suffix = 1 if re.match('^[^@]+@[^@]+\\.[^@]+$', url) else 0\n",
        "            features['prefix_suffix'] = prefix_suffix\n",
        "            random_domain = 1 if re.search('[0-9]{5,}', host) else 0\n",
        "            features['random_domain'] = random_domain\n",
        "            shortening_service = 1 if re.search('bit\\.ly|goo\\.gl|ow\\.ly|t\\.co|tinyurl\\.', url) else 0\n",
        "            features['shortening_service'] = shortening_service\n",
        "            path_extension = 1 if re.search('\\.[a-zA-Z]{3,4}', path) else 0\n",
        "            features['path_extension'] = path_extension\n",
        "            # Send HTTP request to webpage\n",
        "            try:\n",
        "                  response = requests.get(url, timeout=5)\n",
        "                  response.raise_for_status()\n",
        "            except requests.exceptions.Timeout:\n",
        "             features['redirections_number'] = 0\n",
        "             features['external_redirections_number'] = 0\n",
        "             features['phish_hints'] =  0\n",
        "             features['hyperlinks_number'] = 0\n",
        "             features['ratio_intHyperlinks'] = 0\n",
        "             features['ratio_extHyperlinks'] = 0\n",
        "             features['ratio_nullHyperlinks'] = 0\n",
        "             features['extCSS_number'] = 0\n",
        "             features['submit_emails'] = 0\n",
        "             features['ratio_intMedia'] = 0\n",
        "             features['ratio_extMedia'] = 0\n",
        "             features['sfh'] = 0\n",
        "             #continue\n",
        "            except requests.exceptions.HTTPError as error:\n",
        "             features['redirections_number'] = 0\n",
        "             features['external_redirections_number'] = 0\n",
        "             features['phish_hints'] =  0\n",
        "             features['hyperlinks_number'] = 0\n",
        "             features['ratio_intHyperlinks'] = 0\n",
        "             features['ratio_extHyperlinks'] = 0\n",
        "             features['ratio_nullHyperlinks'] = 0\n",
        "             features['extCSS_number'] = 0\n",
        "             features['submit_emails'] = 0\n",
        "             features['ratio_intMedia'] = 0\n",
        "             features['ratio_extMedia'] = 0\n",
        "             features['sfh'] = 0\n",
        "             #continue\n",
        "            except requests.exceptions.RequestException as error:\n",
        "             features['redirections_number'] = 0\n",
        "             features['external_redirections_number'] = 0\n",
        "             features['phish_hints'] =  0\n",
        "             features['hyperlinks_number'] = 0\n",
        "             features['ratio_intHyperlinks'] = 0\n",
        "             features['ratio_extHyperlinks'] = 0\n",
        "             features['ratio_nullHyperlinks'] = 0\n",
        "             features['extCSS_number'] = 0\n",
        "             features['submit_emails'] = 0\n",
        "             features['ratio_intMedia'] = 0\n",
        "             features['ratio_extMedia'] = 0\n",
        "             features['sfh'] = 0\n",
        "            #continue\n",
        "            # Count number of redirections\n",
        "            try:\n",
        "               redirections_number = len(response.history)\n",
        "               features['redirections_number'] = redirections_number\n",
        "            except:\n",
        "               features['redirections_number'] = 0\n",
        "            # Count number of external redirections\n",
        "            try:\n",
        "               external_redirections_number = 0\n",
        "               for resp in response.history:\n",
        "                 url_parsed = urlparse(resp.url)\n",
        "                 if url_parsed.netloc != urlparse(url).netloc:\n",
        "                      external_redirections_number += 1\n",
        "               features['external_redirections_number'] = external_redirections_number\n",
        "            except:\n",
        "                    features['external_redirections_number'] = 0\n",
        "            # Extract domain and paths\n",
        "            url_parsed = urlparse(url)\n",
        "            domain = url_parsed.netloc\n",
        "            path = url_parsed.path\n",
        "\n",
        "            # Extract words from domain and paths\n",
        "            domain_words = re.findall(r'\\w+', domain)\n",
        "            path_words = re.findall(r'\\w+', path)\n",
        "\n",
        "            # Calculate length of raw URL and number of characters repeated\n",
        "            try:\n",
        "               length_words_raw = len(re.findall(r'\\w+', url))\n",
        "               char_repeat = len(re.findall(r'(.)\\1{2,}', url))\n",
        "            except:\n",
        "                 length_words_raw = 0\n",
        "                 char_repeat = 0\n",
        "            features['char_repeat'] = char_repeat\n",
        "            features['length_words_raw'] = length_words_raw\n",
        "\n",
        "            # Find shortest and longest words in domain and paths\n",
        "            shortest_words_raw = ''\n",
        "            shortest_word_host = ''\n",
        "            shortest_word_path = ''\n",
        "            longest_words_raw = ''\n",
        "            longest_word_host = ''\n",
        "            longest_word_path = ''\n",
        "            try:\n",
        "                 shortest_word_host = len(min(domain_words, key=len))\n",
        "                 longest_word_host = len(max(domain_words, key=len))\n",
        "            except:\n",
        "                 shortest_word_host = 0\n",
        "                 longest_word_host = 0\n",
        "            features['shortest_word_host'] = shortest_word_host\n",
        "            features['longest_word_host'] = longest_word_host\n",
        "            try:\n",
        "                 shortest_word_path = len(min(path_words, key=len))\n",
        "                 longest_word_path = len(max(path_words, key=len))\n",
        "            except:\n",
        "                   shortest_word_path = 0\n",
        "                   longest_word_path = 0\n",
        "            features['longest_word_path'] = longest_word_path\n",
        "            features['shortest_word_path'] = shortest_word_path\n",
        "            try:\n",
        "              shortest_words_raw = len(min(domain_words + path_words, key=len))\n",
        "              longest_words_raw = len(max(domain_words + path_words, key=len))\n",
        "            except:\n",
        "                   shortest_words_raw = 0\n",
        "                   longest_words_raw = 0\n",
        "            features['longest_words_raw'] = longest_words_raw\n",
        "            features['shortest_words_raw'] = shortest_words_raw\n",
        "            # Calculate average word lengths in domain and paths\n",
        "            try:\n",
        "                average_word_raw = length_words_raw / (len(domain_words) + len(path_words))\n",
        "            except ZeroDivisionError:\n",
        "                      average_word_raw = 0\n",
        "            features['average_word_raw'] = average_word_raw\n",
        "            try:\n",
        "                  average_word_host = len(domain) / len(domain_words)\n",
        "            except ZeroDivisionError:\n",
        "                     average_word_host = 0\n",
        "            features['average_word_host'] = average_word_host\n",
        "            try:\n",
        "                average_word_path = len(path) / len(path_words)\n",
        "            except ZeroDivisionError:\n",
        "                      average_word_path = 0\n",
        "            features['average_word_path'] = average_word_path\n",
        "\n",
        "            # Check for phishing hints in page content\n",
        "            try:\n",
        "              phish_hints = ''\n",
        "              phish_keywords = ['login', 'password', 'account', 'verify', 'security', 'update', 'confirm']\n",
        "              soup = BeautifulSoup(response.content, 'html.parser')\n",
        "              for keyword in phish_keywords:\n",
        "                if soup.find(text=re.compile(keyword, re.IGNORECASE)):\n",
        "                    phish_hints += keyword + ', '\n",
        "              features['phish_hints'] = len(phish_hints)\n",
        "            except:\n",
        "                  features['phish_hints'] = 0\n",
        "\n",
        "        \"\"\" # Check if domain is present in brand\n",
        "            brand = 'example'\n",
        "            domain_in_brand = domain in brand\n",
        "\n",
        "            # Check if brand is present in subdomain or path\n",
        "            subdomain, domain, suffix = tldextract.extract(url_parsed.netloc)\n",
        "            brand_in_subdomain = brand in subdomain\n",
        "            brand_in_path = brand in path\n",
        "            # Send HTTP request to webpage\n",
        "            response = requests.get(url)\"\"\"\n",
        "        # Count number of hyperlinks\n",
        "        try:\n",
        "             hyperlinks = soup.find_all('a')\n",
        "             hyperlinks_number = len(hyperlinks)\n",
        "             features['hyperlinks_number'] = hyperlinks_number\n",
        "        except:\n",
        "             features['hyperlinks_number'] = 0\n",
        "        # Calculate ratio of internal to external hyperlinks\n",
        "        try:\n",
        "           int_hyperlinks = []\n",
        "           ext_hyperlinks = []\n",
        "           for link in hyperlinks:\n",
        "                   if link.get('href') and not link.get('href').startswith('#') and not link.get('href').startswith('http'):\n",
        "                          int_hyperlinks.append(link)\n",
        "                   elif link.get('href') and link.get('href').startswith('http'):\n",
        "                                  ext_hyperlinks.append(link)\n",
        "           if len(int_hyperlinks) > 0:\n",
        "                      ratio_intHyperlinks = len(int_hyperlinks) / len(hyperlinks_number)\n",
        "           if len(ext_hyperlinks) > 0:\n",
        "                      ratio_extHyperlinks = len(ext_hyperlinks) / len(hyperlinks_number)\n",
        "           features['ratio_intHyperlinks'] = ratio_intHyperlinks\n",
        "           features['ratio_extHyperlinks'] = ratio_extHyperlinks\n",
        "        except:\n",
        "           features['ratio_intHyperlinks'] = 0\n",
        "           features['ratio_extHyperlinks'] = 0\n",
        "        # Count number of null hyperlinks\n",
        "           try:\n",
        "              null_hyperlinks = soup.find_all('a', href=None)\n",
        "              ratio_nullHyperlinks = len(null_hyperlinks) / len(hyperlinks_number)\n",
        "              features['ratio_nullHyperlinks'] = ratio_nullHyperlinks\n",
        "           except:\n",
        "                features['ratio_nullHyperlinks'] = 0\n",
        "        # Count number of external CSS files\n",
        "        try:\n",
        "          external_css = soup.find_all('link', rel='stylesheet', href=lambda href: href and not href.startswith('#'))\n",
        "          extCSS_number = len(external_css)\n",
        "          features['extCSS_number'] = extCSS_number\n",
        "        except:\n",
        "          features['extCSS_number'] = 0\n",
        "        # Submit email to haveibeenpwned API\n",
        "        try:\n",
        "         submit_emails = 0\n",
        "         headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "         response = requests.get(url, headers=headers)\n",
        "         if response.status_code == 200:\n",
        "                     submit_emails = 1\n",
        "         features['submit_emails'] = submit_emails\n",
        "        except:\n",
        "               features['submit_emails'] = 0\n",
        "        # Count number of internal and external media files\n",
        "        try:\n",
        "         ratio_intMedia = 0\n",
        "         ratio_extMedia = 0\n",
        "         int_media = 0\n",
        "         ext_media = 0\n",
        "         response = requests.get(url)\n",
        "         soup = BeautifulSoup(response.content, 'html.parser')\n",
        "         for tag in soup.find_all(['img', 'audio', 'video']):\n",
        "                 if tag.has_attr('src') and not tag['src'].startswith('http'):\n",
        "                                                  int_media += 1\n",
        "                 elif tag.has_attr('src') and tag['src'].startswith('http'):\n",
        "                                                   ext_media += 1\n",
        "         if int_media + ext_media > 0:\n",
        "                  ratio_intMedia = int_media / (int_media + ext_media)\n",
        "                  ratio_extMedia = ext_media / (int_media + ext_media)\n",
        "         features['ratio_intMedia'] = ratio_intMedia\n",
        "         features['ratio_extMedia'] = ratio_extMedia\n",
        "        except:\n",
        "                     features['ratio_intMedia'] = 0\n",
        "                     features['ratio_extMedia'] = 0\n",
        "        # Check for Same-Origin Policy violation\n",
        "        try:\n",
        "         sfh = ''\n",
        "         response = requests.get(url, allow_redirects=False)\n",
        "         if response.status_code == 200:\n",
        "               soup = BeautifulSoup(response.content, 'html.parser')\n",
        "               sfh_tag = soup.find('form', {'target': '_blank'})\n",
        "               if sfh_tag:\n",
        "                      sfh = sfh_tag.get('action')\n",
        "         features['sfh'] = sfh\n",
        "        except:\n",
        "               features['sfh'] = 0\n",
        "        # Return the feature dictionary\n",
        "        return  features\n",
        "        \"\"\"# Calculate ratio of internal to external redirections\n",
        "            int_redirections = []\n",
        "            ext_redirections = []\n",
        "            for tag in soup.find_all():\n",
        "                    if tag.get('meta', {'http-equiv': 'refresh'}) and not tag['content'].startswith('0;url='):\n",
        "                      if tag['content'].startswith('http'):\n",
        "                             ext_redirections.append(tag)\n",
        "                    else:\n",
        "                       int_redirections.append(tag)\n",
        "            if len(int_redirections) > 0:\n",
        "                      ratio_intRedirection = len(int_redirections) / (len(int_redirections) + len(ext_redirections))\n",
        "            if len(ext_redirections) > 0:\n",
        "                      ratio_extRedirection = len(ext_redirections) / (len(int_redirections) + len(ext_redirections))\n",
        "\n",
        "        # Calculate ratio of internal to external errors\n",
        "            int_errors = []\n",
        "            ext_errors = []\n",
        "            for link in hyperlinks:\n",
        "                    if link.get('href') and link.get('href').startswith('http'):\n",
        "                                try:\n",
        "                                      response = requests.get(link.get('href'))\n",
        "                                      if response.status_code == 200:\n",
        "                                                        continue\n",
        "                                      elif response.status_code >= 400 and response.status_code < 500:\n",
        "\n",
        "                                              ext_errors.append(link)\n",
        "                                      else:\n",
        "                                               continue\n",
        "                                except:\n",
        "                                       ext_errors.append(link)\n",
        "                    elif link.get('href') and not link.get('href').startswith('#'):\n",
        "                                    try:\n",
        "                                        response = requests.get(url + link.get('href'))\n",
        "                                        if response.status_code == 200:\n",
        "                                                       continue\n",
        "                                        elif response.status_code >= 400 and response.status_code < 500:\n",
        "                                                  int_errors.append(link)\n",
        "                                        else:\n",
        "                                                 continue\n",
        "                                    except:\n",
        "                                        int_errors.append(link)\n",
        "            if len(int_errors) > 0:\n",
        "                            ratio_intErrors = len(int_errors) / (len(int_errors) + len(ext_errors))\n",
        "            if len(ext_errors) > 0:\n",
        "                            ratio_extErrors = len(ext_errors) / (len(int_errors) + len(ext_errors))\n",
        "\n",
        "            # Check if login form is present\n",
        "            login_form = len(soup.find_all('form', {'class': 'login'})) > 0\n",
        "\n",
        "            # Check if external favicon is used\n",
        "            external_favicon = len(soup.find_all('link', {'rel': 'icon', 'href': lambda href: href.startswith('http')})) > 0\n",
        "\n",
        "            # Count number of links in HTML tags\n",
        "            links_in_tags = len(soup.find_all(lambda tag: tag.name in ['a', 'img', 'script', 'link'] and tag.has_attr('href')))\n",
        "           \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ9LqVMAU-Pt"
      },
      "outputs": [],
      "source": [
        "gf = mailbox.mbox('/content/enron_emails.mbox')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAxxmxhfi2zo"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(columns=['text','lengthOfEmailId', 'noOfDotsInEmailId',\n",
        "'noOfDashesInEmailId', 'noOfSpecialCharsInEmailId', 'noOfDigitsInEmailId',\n",
        "'noOfSubdomainsInEmailId', 'noof_urls','length_url', 'length_hostname','ip','ip_asn',\n",
        "'ip_last_analysis_date','ip_harmless','ip_malicious','ip_suspicious','ip_timeout','ip_undetected','ip_last_modification_date'\n",
        ",'ip_harmless_votes','ip_malicious_votes','first_submission_date','last_analysis_date','harmless','malicious','suspicious','timeout',\n",
        "'undetected','last_http_response_code', 'last_http_response_content_length', 'last_modification_date','harmless_votes','malicious_votes'\n",
        ",'dots_number', 'hyphens_number', 'at_number', 'qm_number', 'and_number',\n",
        "'or_number', 'eq_number','tildes_number','percent_number','slash_number',\n",
        "'star_number','colon_number','comma_number','semicolon_number',\n",
        "'dollar_number','space_number', 'www_number','com_number', 'http_in_path',\n",
        "'https_token','ratio_digits_url','ratio_digits_host', 'punycode', 'port',\n",
        "'tld_in_path','tld_in_subdomain','abnormal_subdomain','subdomains_number',\n",
        "'prefix_suffix','random_domain', 'shortening_service',  'path_extension',\n",
        "'redirections_number','external_redirections_number', 'length_words_raw',\n",
        "'char_repeat','shortest_word_host','longest_word_host','shortest_word_path',\n",
        "'longest_word_path','shortest_words_raw','longest_words_raw',\n",
        " 'average_word_raw','average_word_host','average_word_path','phish_hints',\n",
        "'hyperlinks_number','ratio_intHyperlinks','ratio_extHyperlinks',\n",
        "'ratio_nullHyperlinks','extCSS_number','submit_emails',\n",
        "'ratio_intMedia','ratio_extMedia','sfh','noof_attachements','class label'])\n",
        "stringUtil = StringUtil()\n",
        "for email in gf:\n",
        "    emailParser = EmailParser(email)\n",
        "    no_of_attachments = emailParser.get_no_of_attachments()\n",
        "    emailid_features = stringUtil.process_email_address(emailParser.get_sender_email_address())\n",
        "    urls_features = stringUtil.process_urls(emailParser.get_urls())\n",
        "    word_dict = stringUtil.process_text(emailParser.get_email_text())\n",
        "    #print(urls_features)\n",
        "    new_row = pd.Series([len(word_dict),*emailid_features, *urls_features.values(),\n",
        "                       no_of_attachments,0], index=df1.columns)\n",
        "    df1 = df1.append(new_row, ignore_index=True)\n",
        "\n",
        "#get most common words from phishing emails\n",
        "malicious_words = stringUtil.get_most_common_words()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "PFAklz3ABWZb",
        "outputId": "a9bb388e-e6d8-47b5-e369-6058c0d28a9f"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Es98FyHjBtkQ",
        "outputId": "d9a4ec6c-c6fc-4159-f25b-c0782aaf307f"
      },
      "outputs": [],
      "source": [
        "df1.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B15A4N1QCCH3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# Save the DataFrame as a CSV file\n",
        "df1.to_csv('data_ff.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL9la12fhEi2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "U78SUiZc5cJy",
        "outputId": "48d9418b-ddcc-43c6-c70d-1cb424fcf602"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "oMjaSE29eM6H",
        "outputId": "5edcae71-3f32-4d22-f5f1-bd89cd65c26f"
      },
      "outputs": [],
      "source": [
        "df1.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "iQy0RX_Jkykr",
        "outputId": "ab729b08-e1d8-4055-93b4-1bc7f89cd6cc"
      },
      "outputs": [],
      "source": [
        "df1.describe()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
